import re
from collections import Counter
import sys
sys.path.append('.')
from text_processor import TextProcessor

def analyze_pdf_extraction_detailed(pdf_content: str):
    """PDFì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„¸ ë¶„ì„"""
    
    print("=" * 60)
    print("ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ìƒì„¸ ë¶„ì„")
    print("=" * 60)
    
    # 1. ì›ë³¸ í…ìŠ¤íŠ¸ ì •ë³´
    print(f"1. ì›ë³¸ í…ìŠ¤íŠ¸ ì •ë³´:")
    print(f"   - ì´ ë¬¸ì ìˆ˜: {len(pdf_content):,}")
    print(f"   - ì¤„ ìˆ˜: {len(pdf_content.splitlines()):,}")
    paragraphs = [p for p in pdf_content.split('\n\n') if p.strip()]
    print(f"   - ë‹¨ë½ ìˆ˜: {len(paragraphs):,}")
    
    # 2. ëª¨ë“  ë‹¨ìœ„ ì¶”ì¶œ (ë‹¨ì–´, ìˆ™ì–´, êµ¬ë¬¸)
    print(f"\n2. í…ìŠ¤íŠ¸ ë‹¨ìœ„ ë¶„ì„:")
    
    # ëª¨ë“  í…ìŠ¤íŠ¸ í† í° (ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬)
    all_tokens = pdf_content.split()
    print(f"   - ì „ì²´ í† í° ìˆ˜: {len(all_tokens):,}")
    
    # ì˜ì–´ ë‹¨ì–´ë§Œ ì¶”ì¶œ
    english_words = re.findall(r'\b[a-zA-Z]+\b', pdf_content.lower())
    print(f"   - ì˜ì–´ ë‹¨ì–´ ìˆ˜ (ì¤‘ë³µ í¬í•¨): {len(english_words):,}")
    
    # ê³ ìœ  ì˜ì–´ ë‹¨ì–´
    unique_english_words = set(english_words)
    print(f"   - ê³ ìœ  ì˜ì–´ ë‹¨ì–´ ìˆ˜: {len(unique_english_words):,}")
    
    # ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ìœ„ (ì˜ˆ: "word1", "2nd", "COVID-19")
    alphanumeric = re.findall(r'\b[a-zA-Z0-9]+[a-zA-Z]+[a-zA-Z0-9]*\b', pdf_content.lower())
    print(f"   - ìˆ«ì í¬í•¨ ë‹¨ìœ„: {len(set(alphanumeric)):,}")
    
    # í•˜ì´í”ˆìœ¼ë¡œ ì—°ê²°ëœ ë‹¨ì–´ (ì˜ˆ: "long-term", "well-being")
    hyphenated = re.findall(r'\b[a-zA-Z]+-[a-zA-Z]+(?:-[a-zA-Z]+)*\b', pdf_content.lower())
    print(f"   - í•˜ì´í”ˆ ì—°ê²° ë‹¨ì–´: {len(set(hyphenated)):,}")
    
    # ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ë‹¨ì–´ (ì˜ˆ: "don't", "it's")
    apostrophe_words = re.findall(r"\b[a-zA-Z]+\'[a-zA-Z]+\b", pdf_content.lower())
    print(f"   - ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ë‹¨ì–´: {len(set(apostrophe_words)):,}")
    
    # 3. ê¸¸ì´ë³„ ë¶„í¬
    print(f"\n3. ë‹¨ì–´ ê¸¸ì´ ë¶„í¬:")
    length_dist = Counter(len(word) for word in unique_english_words)
    for length in sorted(length_dist.keys())[:10]:  # ì²˜ìŒ 10ê°œ ê¸¸ì´ë§Œ í‘œì‹œ
        print(f"   - {length}ê¸€ì: {length_dist[length]:,}ê°œ")
    if len(length_dist) > 10:
        print(f"   - ... (ì´ {len(length_dist)}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´)")
    
    # 4. ë¹ˆë„ë³„ ìƒìœ„ ë‹¨ì–´ë“¤
    print(f"\n4. ê°€ì¥ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ (Top 20):")
    word_freq = Counter(english_words)
    for word, count in word_freq.most_common(20):
        print(f"   - {word}: {count}íšŒ")
    
    # 5. íŠ¹ìˆ˜ íŒ¨í„´ ë¶„ì„
    print(f"\n5. íŠ¹ìˆ˜ íŒ¨í„´ ë¶„ì„:")
    
    # ëŒ€ë¬¸ìë¡œë§Œ ëœ ë‹¨ì–´ (ì•½ì–´ ë“±)
    all_caps = [word for word in unique_english_words if word.isupper() and len(word) > 1]
    print(f"   - ëŒ€ë¬¸ì ë‹¨ì–´/ì•½ì–´: {len(all_caps)}ê°œ")
    if all_caps[:10]:
        print(f"     ì˜ˆì‹œ: {', '.join(all_caps[:10])}")
    
    # ë§¤ìš° ê¸´ ë‹¨ì–´ë“¤ (10ê¸€ì ì´ìƒ)
    long_words = [word for word in unique_english_words if len(word) >= 10]
    print(f"   - 10ê¸€ì+ ê¸´ ë‹¨ì–´: {len(long_words)}ê°œ")
    if long_words[:10]:
        print(f"     ì˜ˆì‹œ: {', '.join(sorted(long_words)[:10])}")
    
    # 6. ì²­í‚¹ í›„ ë¶„ì„
    print(f"\n6. ì²­í‚¹ ê³¼ì • ë¶„ì„:")
    tp = TextProcessor()
    clean_text = tp.clean_text(pdf_content)
    chunks = tp.split_text_into_chunks(clean_text)
    
    print(f"   - ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(clean_text):,}")
    print(f"   - ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    print(f"   - í‰ê·  ì²­í¬ ê¸¸ì´: {sum(len(chunk) for chunk in chunks) / len(chunks):.0f} ë¬¸ì")
    
    # ê° ì²­í¬ë³„ ë‹¨ì–´ ìˆ˜
    chunk_word_counts = []
    for i, chunk in enumerate(chunks):
        words_in_chunk = len(re.findall(r'\b[a-zA-Z]+\b', chunk.lower()))
        chunk_word_counts.append(words_in_chunk)
        if i < 3:  # ì²˜ìŒ 3ê°œ ì²­í¬ë§Œ í‘œì‹œ
            print(f"   - ì²­í¬ {i+1}: {words_in_chunk}ê°œ ë‹¨ì–´, {len(chunk)}ë¬¸ì")
    
    total_words_in_chunks = sum(chunk_word_counts)
    print(f"   - ì²­í¬ ë‚´ ì´ ë‹¨ì–´ ìˆ˜: {total_words_in_chunks:,}")
    
    # 7. ì†ì‹¤ ë¶„ì„
    print(f"\n7. ë°ì´í„° ì†ì‹¤ ë¶„ì„:")
    original_word_count = len(english_words)
    chunked_word_count = total_words_in_chunks
    
    print(f"   - ì›ë³¸ ë‹¨ì–´ ìˆ˜: {original_word_count:,}")
    print(f"   - ì²­í‚¹ í›„ ë‹¨ì–´ ìˆ˜: {chunked_word_count:,}")
    print(f"   - ì†ì‹¤ë¥ : {((original_word_count - chunked_word_count) / original_word_count * 100):.1f}%")
    
    return {
        'total_characters': len(pdf_content),
        'total_tokens': len(all_tokens),
        'english_words': len(english_words),
        'unique_words': len(unique_english_words),
        'chunks': len(chunks),
        'words_in_chunks': total_words_in_chunks
    }

def test_with_sample_text():
    """ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸"""
    sample_text = """
    This is a sample PDF content with various types of words.
    It contains regular words, hyphenated-words, contractions like don't and won't.
    There are also ACRONYMS, numbers like 123, and mixed cases like iPhone.
    Some phrases might be considered idioms or phrasal verbs like "look up" or "break down".
    Long words like "internationalization" and "antidisestablishmentarianism" are also included.
    """
    
    print("ìƒ˜í”Œ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸:")
    analyze_pdf_extraction_detailed(sample_text)

if __name__ == "__main__":
    test_with_sample_text()
    
    print("\n" + "="*60)
    print("ì‹¤ì œ PDF í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìœ„í•´ì„œëŠ”:")
    print("ì›¹ì•±ì—ì„œ PDF ì—…ë¡œë“œ ì‹œ ì´ ë¶„ì„ì´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    print("="*60) 