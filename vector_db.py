import chromadb
from chromadb.config import Settings
from openai import AzureOpenAI
import os
import re
from typing import List, Dict
import json

# Try to import sentence_transformers, fallback to a simpler embedding method
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("Warning: sentence_transformers not available, using basic embedding")

class VectorDB:
    def __init__(self, collection_name: str = "rag_documents", model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        ë²¡í„° DB ì´ˆê¸°í™”
        
        Args:
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
            model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                self.model = SentenceTransformer(model_name)
                self.use_sentence_transformers = True
            except Exception as e:
                print(f"Failed to load sentence transformer model: {e}")
                self.use_sentence_transformers = False
                self.model = None
        else:
            self.use_sentence_transformers = False
            self.model = None
            
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        
        # RAGì— ë“±ë¡ëœ ë‹¨ì–´ë“¤ì„ ì €ì¥í•  ì‚¬ì „
        self.vocabulary = set()
        self._load_vocabulary()
        
        self.azure_embed_client = None
        self.azure_embed_deployment = None

        ak = os.getenv("AOAI_API_KEY")
        ae = os.getenv("AOAI_ENDPOINT")
        av = os.getenv("AOAI_API_VERSION")
        ad = os.getenv("AOAI_EMBEDDING_DEPLOYMENT")
        if ak and ae and av and ad:
            try:
                self.azure_embed_client = AzureOpenAI(
                    api_key=ak,
                    api_version=av,
                    azure_endpoint=ae,
                )
                self.azure_embed_deployment = ad
            except Exception as e:
                print(f"Azure Embedding client init failed: {e}")
                self.azure_embed_client = None
                self.azure_embed_deployment = None
    
    def add_documents(self, texts: List[str], metadatas: List[Dict] = None):
        """
        ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì¶”ê°€
        
        Args:
            texts: ì¶”ê°€í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            metadatas: ê° í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if metadatas is None:
            metadatas = [{"source": f"doc_{i}"} for i in range(len(texts))]
        
        # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        if self.use_sentence_transformers:
            embeddings = self.model.encode(texts).tolist()
        else: 
            embeddings = self._azure_embed(texts)
            
        
        # ê³ ìœ  ID ìƒì„±
        ids = [f"doc_{i}_{hash(text)}" for i, text in enumerate(texts)]
        
        # ChromaDBì— ì¶”ê°€
        self.collection.add(
            embeddings=embeddings,
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
        
        # ìƒˆë¡œìš´ ë‹¨ì–´ë“¤ì„ ì–´íœ˜ì— ì¶”ê°€
        self._extract_and_add_vocabulary(texts)
        
        print(f"{len(texts)}ê°œì˜ ë¬¸ì„œê°€ ë²¡í„° DBì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"í˜„ì¬ ì–´íœ˜ í¬ê¸°: {len(self.vocabulary)}ê°œ ë‹¨ì–´")
    
    def search(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            n_results: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        if self.use_sentence_transformers:
            query_embedding = self.model.encode([query]).tolist()
        else:
            query_embedding = self._azure_embed([query])
        
        # ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=n_results,
            include=['documents', 'metadatas', 'distances']
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for i in range(len(results['documents'][0])):
            formatted_results.append({
                'document': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i]
            })
        
        return formatted_results
    
    def get_collection_info(self):
        """ì»¬ë ‰ì…˜ ì •ë³´ ë°˜í™˜"""
        return {
            'count': self.collection.count(),
            'name': self.collection.name
        }
    
    def clear_collection(self):
        """ì»¬ë ‰ì…˜ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
            try:
                self.client.delete_collection(self.collection.name)
            except Exception as e:
                print(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
            
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            self.collection = self.client.get_or_create_collection(
                name=self.collection.name,
                metadata={"hnsw:space": "cosine"}
            )
            
            # ì–´íœ˜ ì´ˆê¸°í™”
            self.vocabulary = set()
            self._save_vocabulary()
            
            print("ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            # ì™„ì „íˆ ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ ìƒì„± ì‹œë„
            try:
                import time
                new_collection_name = f"{self.collection.name}_{int(time.time())}"
                self.collection = self.client.get_or_create_collection(
                    name=new_collection_name,
                    metadata={"hnsw:space": "cosine"}
                )
                self.vocabulary = set()
                self._save_vocabulary()
                print(f"ìƒˆ ì»¬ë ‰ì…˜ '{new_collection_name}'ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e2:
                print(f"ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e2}")
                raise e2
    
    
    def _load_vocabulary(self):
        """ì €ì¥ëœ ì–´íœ˜ íŒŒì¼ì„ ë¡œë“œ"""
        vocab_file = "./chroma_db/vocabulary.txt"
        try:
            if os.path.exists(vocab_file):
                with open(vocab_file, 'r', encoding='utf-8') as f:
                    self.vocabulary = set(line.strip().lower() for line in f if line.strip())
                print(f"ì–´íœ˜ ë¡œë“œ ì™„ë£Œ: {len(self.vocabulary)}ê°œ ë‹¨ì–´")
        except Exception as e:
            print(f"ì–´íœ˜ ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.vocabulary = set()
    
    def _save_vocabulary(self):
        """ì–´íœ˜ë¥¼ íŒŒì¼ì— ì €ì¥"""
        vocab_file = "./chroma_db/vocabulary.txt"
        try:
            os.makedirs("./chroma_db", exist_ok=True)
            with open(vocab_file, 'w', encoding='utf-8') as f:
                for word in sorted(self.vocabulary):
                    f.write(f"{word}\n")
            print(f"ì–´íœ˜ ì €ì¥ ì™„ë£Œ: {len(self.vocabulary)}ê°œ ë‹¨ì–´")
        except Exception as e:
            print(f"ì–´íœ˜ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _extract_and_add_vocabulary(self, texts: List[str]):
        """í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ì—¬ ì–´íœ˜ì— ì¶”ê°€ (êµìœ¡ìš© êµì¬ ìµœì í™”)"""
        import re
        
        initial_vocab_size = len(self.vocabulary)
        new_words = []
        total_extracted_words = 0
        
        for text in texts:
            # 1. ìˆœìˆ˜ ì˜ì–´ ë‹¨ì–´
            english_words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
            
            # 2. í•˜ì´í”ˆ ì—°ê²° ë‹¨ì–´ (phrasal verbs, compound words)
            hyphenated_words = re.findall(r'\b[a-zA-Z]+-[a-zA-Z]+(?:-[a-zA-Z]+)*\b', text.lower())
            
            # 3. ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ë‹¨ì–´ (contractions)
            apostrophe_words = re.findall(r"\b[a-zA-Z]+\'[a-zA-Z]+\b", text.lower())
            
            # 4. êµìœ¡ìš© êµì¬ í˜•ì‹ ì²˜ë¦¬: "171 attention ì£¼ì˜" â†’ "attention"
            # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ì—ì„œ ì•ì˜ ìˆ«ì ì œê±°
            numbered_words = re.findall(r'\b\d+[a-zA-Z]+\b', text.lower())
            cleaned_numbered_words = []
            for word in numbered_words:
                # ì•ì˜ ìˆ«ì ì œê±°í•˜ê³  ì˜ì–´ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                clean_word = re.sub(r'^\d+', '', word)
                if len(clean_word) >= 2:  # ìµœì†Œ 2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ
                    cleaned_numbered_words.append(clean_word)
            
            # 5. ëŒ€ë¬¸ì ì•½ì–´ (ì˜ˆ: "USA", "PDF", "API")
            acronyms = re.findall(r'\b[A-Z]{2,}\b', text)
            
            # ëª¨ë“  ë‹¨ì–´ í•©ì¹˜ê¸°
            all_words = english_words + hyphenated_words + apostrophe_words + cleaned_numbered_words + [a.lower() for a in acronyms]
            
            # ì¤‘ë³µ ì œê±°
            unique_words = list(set(all_words))
            total_extracted_words += len(unique_words)
            
            # ëª¨ë“  ë‹¨ì–´ë¥¼ ì–´íœ˜ì— ì¶”ê°€
            for word in unique_words:
                if word not in self.vocabulary:
                    new_words.append(word)
                self.vocabulary.add(word)
        
        # ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥
        print(f"ğŸ“š êµìœ¡ìš© êµì¬ ì–´íœ˜ ì¶”ì¶œ ê²°ê³¼:")
        print(f"   - ì´ ì¶”ì¶œëœ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {total_extracted_words:,}")
        print(f"   - ìƒˆë¡œ ì¶”ê°€ëœ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {len(new_words):,}")
        print(f"   - ê¸°ì¡´ ì–´íœ˜ í¬ê¸°: {initial_vocab_size:,}")
        print(f"   - í˜„ì¬ ì–´íœ˜ í¬ê¸°: {len(self.vocabulary):,}")
        
        if len(new_words) > 0:
            # ìˆœìˆ˜ ì˜ì–´ ë‹¨ì–´ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ í‘œì‹œ
            pure_english = [w for w in new_words if re.match(r'^[a-zA-Z]+$', w)]
            print(f"   - ìˆœìˆ˜ ì˜ì–´ ë‹¨ì–´ ì˜ˆì‹œ (ì²˜ìŒ 20ê°œ): {sorted(pure_english)[:20]}")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
            categories = {
                '1ê¸€ì': [w for w in new_words if len(w) == 1],
                '2ê¸€ì': [w for w in new_words if len(w) == 2],
                '3-5ê¸€ì': [w for w in new_words if 3 <= len(w) <= 5],
                '6-10ê¸€ì': [w for w in new_words if 6 <= len(w) <= 10],
                '10ê¸€ì+': [w for w in new_words if len(w) > 10],
                'í•˜ì´í”ˆë‹¨ì–´': [w for w in new_words if '-' in w],
                'ì¶•ì•½í˜•': [w for w in new_words if "'" in w],
                'ëŒ€ë¬¸ìì•½ì–´': [w for w in new_words if w.isupper() and len(w) > 1]
            }
            
            for category, words in categories.items():
                if words:
                    print(f"   - {category}: {len(words)}ê°œ (ì˜ˆ: {', '.join(words[:3])})")
        
        # ë³€ê²½ì‚¬í•­ ì €ì¥
        self._save_vocabulary()
    
    def get_vocabulary(self) -> List[str]:
        """ì „ì²´ ì–´íœ˜ ë°˜í™˜"""
        return sorted(list(self.vocabulary))
    
    def get_filtered_vocabulary(self, keywords: str, context_documents: List[str] = None) -> List[str]:
        """í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ì–´íœ˜ë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜"""
        # í‚¤ì›Œë“œì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
        keyword_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', keywords.lower()))
        
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œì—ì„œ ë‹¨ì–´ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
        context_words = set()
        if context_documents:
            for doc in context_documents:
                context_words.update(re.findall(r'\b[a-zA-Z]{3,}\b', doc.lower()))
        
        # ê´€ë ¨ ë‹¨ì–´ë“¤ í•„í„°ë§
        relevant_words = set()
        
        # í‚¤ì›Œë“œ ìì²´ í¬í•¨
        relevant_words.update(keyword_words & self.vocabulary)
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ë‹¨ì–´ë“¤ í¬í•¨
        relevant_words.update(context_words & self.vocabulary)
        
        # í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ ì¶”ê°€ (ê°„ë‹¨í•œ ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­)
        for vocab_word in self.vocabulary:
            for keyword in keyword_words:
                if keyword in vocab_word or vocab_word in keyword:
                    relevant_words.add(vocab_word)
                    break
        
        return sorted(list(relevant_words)) 

    # ì¶”ê°€: Azure ì„ë² ë”© í•¨ìˆ˜
    def _azure_embed(self, texts: List[str]) -> List[List[float]]:
        if not self.azure_embed_client or not self.azure_embed_deployment:
            raise RuntimeError("Azure embedding client not configured")

        # 1) ì…ë ¥ ì •ì œ: ë¬¸ìì—´í™”, ê³µë°±/ë¹ˆë¬¸ì ì œê±°, ê¸¸ì´ ì œí•œ
        clean: List[str] = []
        for t in texts:
            if t is None:
                continue
            s = str(t).replace("\x00", "").strip()
            if not s:
                continue
            if len(s) > 8000:
                s = s[:8000]
            clean.append(s)

        if not clean:
            raise ValueError("No valid text to embed")

        # 2) ë°°ì¹˜ í˜¸ì¶œ â†’ ì‹¤íŒ¨ ì‹œ ë‹¨ê±´ ì¬ì‹œë„
        try:
            resp = self.azure_embed_client.embeddings.create(
                model=self.azure_embed_deployment,
                input=clean  # list[str]
            )
            return [d.embedding for d in resp.data]
        except Exception as e:
            # ì¼ë¶€ í™˜ê²½ì—ì„œ ë°°ì¹˜ ì…ë ¥ í˜•ì‹ ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆì–´ ë‹¨ê±´ìœ¼ë¡œ ì¬ì‹œë„
            try:
                out: List[List[float]] = []
                for s in clean:
                    r = self.azure_embed_client.embeddings.create(
                        model=self.azure_embed_deployment,
                        input=s  # single str
                    )
                    out.append(r.data[0].embedding)
                return out
            except Exception as e2:
                raise RuntimeError(f"Azure embedding failed: {e2}") from e
            