import os
import re
from typing import List, Dict
import pandas as pd

# PDF ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import PyPDF2
    import pdfplumber
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("Warning: PDF libraries not available. Install PyPDF2 and pdfplumber for PDF support.")

class TextProcessor:
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            chunk_size: ê° ì²­í¬ì˜ ìµœëŒ€ ë¬¸ì ìˆ˜
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def read_file(self, file_path: str) -> str:
        """
        íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì½ê¸° (PDF ì§€ì› í¬í•¨)
        
        Args:
            file_path: ì½ì„ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            íŒŒì¼ ë‚´ìš©
        """
        try:
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì²˜ë¦¬
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext == '.txt':
                with open(file_path, 'r', encoding='utf-8') as file:
                    return file.read()
            elif file_ext == '.csv':
                df = pd.read_csv(file_path)
                return df.to_string(index=False)
            elif file_ext == '.pdf':
                return self._read_pdf(file_path)
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì²˜ë¦¬
                with open(file_path, 'r', encoding='utf-8') as file:
                    return file.read()
                    
        except Exception as e:
            print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            return ""
    
    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ë¦¬
        
        Args:
            text: ì •ë¦¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ë³€ê²½
        text = re.sub(r'\n+', '\n', text)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€ê²½
        text = re.sub(r'\s+', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• 
        sentences = re.split(r'[.!?]\s+', text)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´ í™•ì¸
            test_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(test_chunk) <= self.chunk_size:
                current_chunk = test_chunk
            else:
                # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ìˆëŠ” ì²­í¬ ìƒì„± (ë” ë‚˜ì€ ê²€ìƒ‰ì„ ìœ„í•´)
        overlapped_chunks = []
        for i, chunk in enumerate(chunks):
            overlapped_chunks.append(chunk)
            
            # ë‹¤ìŒ ì²­í¬ì™€ ê²¹ì¹˜ëŠ” ë¶€ë¶„ ìƒì„±
            if i < len(chunks) - 1 and self.chunk_overlap > 0:
                overlap_text = chunk[-self.chunk_overlap:] + " " + chunks[i+1][:self.chunk_overlap]
                overlapped_chunks.append(overlap_text.strip())
        
        return [chunk for chunk in overlapped_chunks if chunk.strip()]
    
    def extract_keywords(self, text: str) -> List[str]:
        """
        Extract English keywords from text
        
        Args:
            text: Text to extract keywords from
            
        Returns:
            List of extracted keywords
        """
        # Keep only English letters, numbers, and spaces
        clean_text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
        
        # Split into words
        words = clean_text.split()
        
        # Remove short words (less than 3 characters for English)
        keywords = [word for word in words if len(word) >= 3]
        
        # Common English stop words to exclude
        stop_words = {'the', 'and', 'are', 'was', 'were', 'been', 'have', 'has', 'had', 
                     'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this',
                     'that', 'these', 'those', 'with', 'for', 'from', 'they', 'them',
                     'their', 'there', 'where', 'when', 'what', 'who', 'how', 'why',
                     'but', 'not', 'all', 'any', 'her', 'him', 'his', 'she', 'you',
                     'your', 'our', 'out', 'one', 'two', 'now', 'new', 'old', 'get'}
        
        # Remove stop words, duplicates, and convert to lowercase
        keywords = list(set([word.lower() for word in keywords if word.lower() not in stop_words]))
        
        return keywords
    
    def process_file(self, file_path: str) -> Dict:
        """
        íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ì™€ ë©”íƒ€ë°ì´í„° ìƒì„±
        
        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # íŒŒì¼ ì½ê¸°
        text = self.read_file(file_path)
        
        if not text:
            return {"chunks": [], "metadata": []}
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        clean_text = self.clean_text(text)
        
        # ì²­í¬ë¡œ ë¶„í• 
        chunks = self.split_text_into_chunks(clean_text)
        
        # ê° ì²­í¬ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = []
        for i, chunk in enumerate(chunks):
            keywords = self.extract_keywords(chunk)
            # ChromaDBëŠ” ë©”íƒ€ë°ì´í„° ê°’ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
            keywords_str = ", ".join(keywords) if keywords else ""
            metadata.append({
                "source": file_path,
                "chunk_id": i,
                "keywords": keywords_str,
                "chunk_length": len(chunk)
            })
        
        return {
            "chunks": chunks,
            "metadata": metadata
        }
    
    def _read_pdf(self, file_path: str) -> str:
        """
        PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        if not PDF_AVAILABLE:
            print("PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install PyPDF2 pdfplumber")
            return ""
        
        text = ""
        
        # ë°©ë²• 1: pdfplumber ì‚¬ìš© (ë” ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
        try:
            with pdfplumber.open(file_path) as pdf:
                print(f"ğŸ“„ PDF ë¶„ì„ ì‹œì‘: {len(pdf.pages)}í˜ì´ì§€")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                        print(f"   í˜ì´ì§€ {page_num}: {len(page_text)} ë¬¸ì")
            
            if text.strip():
                print(f"âœ… pdfplumberë¡œ PDF ì½ê¸° ì„±ê³µ: {len(text):,} ë¬¸ì")
                
                # ìƒì„¸ ë¶„ì„ ì‹¤í–‰
                self._analyze_pdf_content(text)
                return text
                
        except Exception as e:
            print(f"pdfplumber ì˜¤ë¥˜: {e}")
        
        # ë°©ë²• 2: PyPDF2 ì‚¬ìš© (ë°±ì—…)
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            
            if text.strip():
                print(f"âœ… PyPDF2ë¡œ PDF ì½ê¸° ì„±ê³µ: {len(text):,} ë¬¸ì")
                self._analyze_pdf_content(text)
                return text
                
        except Exception as e:
            print(f"PyPDF2 ì˜¤ë¥˜: {e}")
        
        print("âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
        return ""
    
    def _analyze_pdf_content(self, pdf_content: str):
        """PDF ë‚´ìš© ìƒì„¸ ë¶„ì„"""
        try:
            print(f"\nğŸ“Š PDF ë‚´ìš© ìƒì„¸ ë¶„ì„:")
            
            # ìƒ˜í”Œ ë‚´ìš© í‘œì‹œ (ì²˜ìŒ 500ì)
            sample_content = pdf_content[:500].replace('\n', ' ').strip()
            print(f"   - ìƒ˜í”Œ ë‚´ìš© (ì²˜ìŒ 500ì):")
            print(f"     '{sample_content}...'")
            print()
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ í† í° (ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬)
            all_tokens = pdf_content.split()
            print(f"   - ì „ì²´ í† í° ìˆ˜: {len(all_tokens):,}")
            
            # ì˜ì–´ ë‹¨ì–´ë§Œ ì¶”ì¶œ
            english_words = re.findall(r'\b[a-zA-Z]+\b', pdf_content.lower())
            print(f"   - ì˜ì–´ ë‹¨ì–´ ìˆ˜ (ì¤‘ë³µ í¬í•¨): {len(english_words):,}")
            
            # ê³ ìœ  ì˜ì–´ ë‹¨ì–´
            unique_english_words = set(english_words)
            print(f"   - ê³ ìœ  ì˜ì–´ ë‹¨ì–´ ìˆ˜: {len(unique_english_words):,}")
            
            # í•˜ì´í”ˆìœ¼ë¡œ ì—°ê²°ëœ ë‹¨ì–´ (phrasal verbs, compound words)
            hyphenated = re.findall(r'\b[a-zA-Z]+-[a-zA-Z]+(?:-[a-zA-Z]+)*\b', pdf_content.lower())
            hyphenated_unique = set(hyphenated)
            print(f"   - í•˜ì´í”ˆ ì—°ê²° ë‹¨ì–´: {len(hyphenated_unique):,}")
            if hyphenated_unique:
                print(f"     ì˜ˆì‹œ: {', '.join(list(hyphenated_unique)[:5])}")
            
            # ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ë‹¨ì–´ (contractions)
            apostrophe_words = re.findall(r"\b[a-zA-Z]+\'[a-zA-Z]+\b", pdf_content.lower())
            apostrophe_unique = set(apostrophe_words)
            print(f"   - ì¶•ì•½í˜• ë‹¨ì–´: {len(apostrophe_unique):,}")
            if apostrophe_unique:
                print(f"     ì˜ˆì‹œ: {', '.join(list(apostrophe_unique)[:5])}")
            
            # ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ìœ„
            alphanumeric = re.findall(r'\b[a-zA-Z0-9]*[a-zA-Z]+[a-zA-Z0-9]*\b', pdf_content.lower())
            alphanumeric_unique = set(alphanumeric) - unique_english_words  # ìˆœìˆ˜ ì˜ì–´ ë‹¨ì–´ ì œì™¸
            print(f"   - ìˆ«ì í¬í•¨ ë‹¨ìœ„: {len(alphanumeric_unique):,}")
            if alphanumeric_unique:
                print(f"     ì˜ˆì‹œ: {', '.join(list(alphanumeric_unique)[:10])}")
            
            # ëŒ€ë¬¸ì ë‹¨ì–´/ì•½ì–´
            all_caps = re.findall(r'\b[A-Z]{2,}\b', pdf_content)
            print(f"   - ëŒ€ë¬¸ì ë‹¨ì–´/ì•½ì–´: {len(set(all_caps)):,}")
            if all_caps:
                print(f"     ì˜ˆì‹œ: {', '.join(list(set(all_caps))[:5])}")
            
            # ê¸¸ì´ë³„ ë¶„í¬
            from collections import Counter
            length_dist = Counter(len(word) for word in unique_english_words)
            print(f"   - ë‹¨ì–´ ê¸¸ì´ ë¶„í¬: 1ê¸€ì({length_dist[1]}), 2ê¸€ì({length_dist[2]}), 3ê¸€ì({length_dist[3]}), 4ê¸€ì({length_dist[4]}), 5ê¸€ì({length_dist[5]}), 6+ê¸€ì({sum(count for length, count in length_dist.items() if length >= 6)})")
            
            # ì²­í‚¹ í›„ ë¶„ì„
            clean_text = self.clean_text(pdf_content)
            chunks = self.split_text_into_chunks(clean_text)
            
            chunk_word_counts = []
            for chunk in chunks:
                words_in_chunk = len(re.findall(r'\b[a-zA-Z]+\b', chunk.lower()))
                chunk_word_counts.append(words_in_chunk)
            
            total_words_in_chunks = sum(chunk_word_counts)
            
            print(f"   - ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
            print(f"   - ì²­í¬ ë‚´ ì´ ë‹¨ì–´ ìˆ˜: {total_words_in_chunks:,}")
            
            # ì†ì‹¤ ë¶„ì„
            original_word_count = len(english_words)
            loss_rate = ((original_word_count - total_words_in_chunks) / original_word_count * 100) if original_word_count > 0 else 0
            print(f"   - ì²˜ë¦¬ ê³¼ì • ì†ì‹¤ë¥ : {loss_rate:.1f}%")
            
            # ê°€ëŠ¥í•œ ì´ ì–´íœ˜ ìˆ˜ ê³„ì‚°
            total_vocabulary = len(unique_english_words) + len(hyphenated_unique) + len(apostrophe_unique) + len(alphanumeric_unique) + len(set(all_caps))
            print(f"   - ì˜ˆìƒ ì´ ì–´íœ˜ ìˆ˜: {total_vocabulary:,} (ì˜ì–´ë‹¨ì–´ + í•˜ì´í”ˆë‹¨ì–´ + ì¶•ì•½í˜• + ìˆ«ìí¬í•¨ + ì•½ì–´)")
            
            print()
            
        except Exception as e:
            print(f"PDF ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}") 